{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaemi\\anaconda3\\envs\\py_cpu\\lib\\site-packages\\seaborn\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpalettes\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrelational\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mregression\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcategorical\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaemi\\anaconda3\\envs\\py_cpu\\lib\\site-packages\\seaborn\\relational.py:17\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_oldcore\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     VectorPlotter,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     locator_to_legend_entries,\n\u001b[0;32m     13\u001b[0m     adjust_legend_subtitles,\n\u001b[0;32m     14\u001b[0m     _default_color,\n\u001b[0;32m     15\u001b[0m     _deprecate_ci,\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_statistics\u001b[39;00m \u001b[39mimport\u001b[39;00m EstimateAggregator\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39maxisgrid\u001b[39;00m \u001b[39mimport\u001b[39;00m FacetGrid, _facet_docs\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_docstrings\u001b[39;00m \u001b[39mimport\u001b[39;00m DocstringComponents, _core_docs\n",
      "File \u001b[1;32mc:\\Users\\jaemi\\anaconda3\\envs\\py_cpu\\lib\\site-packages\\seaborn\\_statistics.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m gaussian_kde\n\u001b[0;32m     32\u001b[0m     _no_scipy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jaemi\\anaconda3\\envs\\py_cpu\\lib\\site-packages\\scipy\\stats\\__init__.py:485\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    486\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[0;32m    487\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jaemi\\anaconda3\\envs\\py_cpu\\lib\\site-packages\\scipy\\stats\\_stats_py.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[1;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[0;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     49\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32mc:\\Users\\jaemi\\anaconda3\\envs\\py_cpu\\lib\\site-packages\\scipy\\stats\\distributions.py:10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _continuous_distns\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_continuous_distns\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jaemi\\anaconda3\\envs\\py_cpu\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_ksstats\u001b[39;00m \u001b[39mimport\u001b[39;00m kolmogn, kolmognp, kolmogni\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_constants\u001b[39;00m \u001b[39mimport\u001b[39;00m (_XMIN, _EULER, _ZETA3, _SQRT_PI,\n\u001b[0;32m     31\u001b[0m                          _SQRT_2_OVER_PI, _LOG_SQRT_2_OVER_PI)\n\u001b[1;32m---> 32\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_boost\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_boost\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimize\u001b[39;00m \u001b[39mimport\u001b[39;00m root_scalar\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m FitError\n",
      "File \u001b[1;32mc:\\Users\\jaemi\\anaconda3\\envs\\py_cpu\\lib\\site-packages\\scipy\\stats\\_boost\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_boost\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbeta_ufunc\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     _beta_pdf, _beta_cdf, _beta_sf, _beta_ppf,\n\u001b[0;32m      3\u001b[0m     _beta_isf, _beta_mean, _beta_variance,\n\u001b[0;32m      4\u001b[0m     _beta_skewness, _beta_kurtosis_excess,\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_boost\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbinom_ufunc\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     _binom_pdf, _binom_cdf, _binom_sf, _binom_ppf,\n\u001b[0;32m      9\u001b[0m     _binom_isf, _binom_mean, _binom_variance,\n\u001b[0;32m     10\u001b[0m     _binom_skewness, _binom_kurtosis_excess,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_boost\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnbinom_ufunc\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     _nbinom_pdf, _nbinom_cdf, _nbinom_sf, _nbinom_ppf,\n\u001b[0;32m     15\u001b[0m     _nbinom_isf, _nbinom_mean, _nbinom_variance,\n\u001b[0;32m     16\u001b[0m     _nbinom_skewness, _nbinom_kurtosis_excess,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_boost\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhypergeom_ufunc\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     _hypergeom_pdf, _hypergeom_cdf, _hypergeom_sf, _hypergeom_ppf,\n\u001b[0;32m     21\u001b[0m     _hypergeom_isf, _hypergeom_mean, _hypergeom_variance,\n\u001b[0;32m     22\u001b[0m     _hypergeom_skewness, _hypergeom_kurtosis_excess,\n\u001b[0;32m     23\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:398\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Embedding, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/ratings_test.txt', <http.client.HTTPMessage at 0x24696e27640>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", \n",
    "                           filename=\"./data/ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", \n",
    "                           filename=\"./data/ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('./data/ratings_train.txt')\n",
    "test_data = pd.read_table('./data/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "    # 1. 비어 있는 리뷰 처리\n",
    "    # 2. 한글외의 문자는 Nan 처리\n",
    "    # 3. 같은 리뷰 제거  \n",
    "    \n",
    "#  학습 데이터와 테스트 데이터로 정형화\n",
    "    # 4. x 와 y로 분리\n",
    "    # 5. 단어로 토큰 \n",
    "    # 6. 패딩\n",
    "    \n",
    "# 7. 모델 구조 설계\n",
    "# 8. 모델 실행\n",
    "# 9. 모델 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "    # 1. 비어 있는 리뷰 처리\n",
    "    # 2. 한글외의 문자는 Nan 처리\n",
    "    # 3. 같은 리뷰 제거\n",
    "\n",
    "# 1. 비어 있는 리뷰 처리   \n",
    "train_data = train_data.dropna(how='any')  # null 존재하는 행 제거 \n",
    "# 2. 한글을 제외한 문자를 '' 로 변경\n",
    "train_data['document'] = train_data['document'].str.replace(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 \" \"]', \n",
    "                                                          '', regex=True)\n",
    "# '' 를 np.nan  으로 변경 -> nan 을 제거\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data.dropna(inplace=True)\n",
    "# 3. 같은 리뷰 제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  학습 데이터와 테스트 데이터로 정형화\n",
    "    # 4. x 와 y로 분리\n",
    "    # 5. 단어로 토큰 \n",
    "    # 6. 패딩\n",
    "\n",
    "# 4. x 와 y로 분리\n",
    "x_train = train_data['document']\n",
    "y_train = train_data['label']\n",
    "\n",
    "x_test = test_data['document']\n",
    "y_test = test_data['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 자연어 전처리 -> 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(x_train) \n",
    "\n",
    "# 6. 서로 다른 길이의 데이터를 padding -> 10로 \n",
    "padd_x = token.texts_to_sequences(x_train)   # 단어를 토큰화 하고 인덱스만 가져옴\n",
    "\n",
    "# 서로 다른 길이의 데이터를 padding -> 10로 \n",
    "padded_x = pad_sequences(padd_x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 8)             2396144   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 80)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,396,225\n",
      "Trainable params: 2,396,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "4491/4491 [==============================] - 270s 60ms/step - loss: 0.4863 - accuracy: 0.7591\n",
      "Epoch 2/20\n",
      "3963/4491 [=========================>....] - ETA: 29s - loss: 0.2508 - accuracy: 0.9151"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m#  8. model 실행 옵션 설정\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 13\u001b[0m model\u001b[39m.\u001b[39;49mfit(padded_x, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\jaemi\\anaconda3\\envs\\py_cpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jaemi\\anaconda3\\envs\\py_cpu\\lib\\site-packages\\keras\\engine\\training.py:1690\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1688\u001b[0m \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1689\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m-> 1690\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39;49mstep_increment\n\u001b[0;32m   1691\u001b[0m callbacks\u001b[39m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1692\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\jaemi\\anaconda3\\envs\\py_cpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:1395\u001b[0m, in \u001b[0;36mDataHandler.step_increment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1392\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m steps_remaining\n\u001b[0;32m   1393\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution\u001b[39m.\u001b[39massign(original_spe)\n\u001b[1;32m-> 1395\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m   1396\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_increment\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1397\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"The number to increment the step for `on_batch_end` methods.\"\"\"\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step_increment\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 임베딩에 입력된 단어의 수를 지정\n",
    "word_size = len(token.word_index) + 1\n",
    "\n",
    "# 7. 단어 임베딩을 포함한 딥러닝 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_size, 8, input_length=10))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#  8. model 실행 옵션 설정\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(padded_x, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 검증하기 위한 테스트 자료\n",
    "# 서로 다른 길이의 데이터를 padding -> 10로 \n",
    "padd_xt = token.texts_to_sequences(x_test)   # 단어를 토큰화 하고 인덱스만 가져옴\n",
    "\n",
    "# 서로 다른 길이의 데이터를 padding -> 10로 \n",
    "padded_xt = pad_sequences(padd_xt, 10)\n",
    "model.evaluate(padded_xt, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfdml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
